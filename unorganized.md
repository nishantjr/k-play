#### Is there a flag I can pass to see the smt queries generated by the haskell backend?

You can pass `--solver-transcript filename.smt` to the `kore-exec`. If you're
using `kprove` or `krun` instead of calling the backend directly, you can set
`KORE_EXEC_OPTS='--solver-transcript ...'` in your environment.

#### Detecting non-determinism

On the LLVM backend the `-Wuseless` flag to `kompile` will tell the backend to
emit warnings when two rules have left-hand-sides that overlap

* Does this work for functions as well as ordinary rules?

#### Non-exhaustiveness of function rules

The `[functional]` attribute enables this check for the LLVM backend

#### How to profile a semantics with perf with the goal of trying to make it faster

The first thing to be aware of is in order to get meaningful data, you need to
build the semantics and all of its dependencies with optimizations enabled but
without the frame pointer elimination optimization.

For example, for EVM, this means rebuilding GMP, MPFR, JEMalloc, Crypto++,
SECP256K1, etc with the
`export CFLAGS="-g -O2 -fno-omit-frame-pointer"; export CXXFLAGS="-g -O2 -fno-omit-frame-pointer"`
Of the dependencies you listed above as needing to be compiled with
-fno-omit-frame-pointer, GMP, MPFR, and JEMalloc all come from the LLVM backend
and so would apply to all definitions, and not just KEVM.

You can skip this step, but if you do, any samples within these libraries will
not have correct stack trace information, which means you will likely not get a
meaningful set of data that will tell you where the majority of time is really
being spent. Don't worry about rebuilding literally every single dependency
though. Just focus on the ones that you expect to take a non-negligible amount
of runtime. You will be able to tell if you haven't done enough later, and you
can go back and rebuild more.

Once this is done, you then build K with optimizations and debug info enabled,
like so: `mvn package -Dproject.build.type="RelWithDebInfo"` Next, you build the
semantics with optimizations and debug info enabled (i.e., `kompile -O3 -ccopt -g`)

Once all this is done, you should be ready to profile your application.
Essentially, you should run whatever test suite you usually run, but with perf
record -g -- prefixed to the front. For example, for KEVM it's
`perf record -g -- make test-conformance`. For best data, don't run this step in
parallel.

Finally, you want to filter out just the samples that landed within the llvm
backend and view the report. for this, you need to know the name of the binary
that was generated by your build system. Normally it is `interpreter`, but e.g.Â if
you are building the web3 client for kevm, it would be `kevm-client`. You will
want to run `perf report -g -c $binary_name` . If all goes well, you should see
a breakdown of where CPU time has been spent executing the application. You will
know that sufficient time was spent rebuilding dependencies with the correct
flags when the total time reported by the main method is close to 100%. If it's
not close to 100%, this is probably because a decent amount of self time was
reported in stack traces that were not built with frame pointers enabled,
meaning that perf was unable to walk the stack. You will have to go back,
rebuild the appropriate libraries, and then record your trace again.

Your ultimate goal is to identify the hotspots that take the most time, and make
them execute faster. `step` and `step_1234` like functions refer to the cost of
matching. `side_condition_1234` is a side condition. `apply_rule_1234` is
constructing the rhs of a rule. You can convert from this rule ordinal to a
location using the `llvm-kompile-compute-loc` script in the bin folder of the
llvm backend repo. For example,
`llvm-kompile-compute-loc 5868 evm-semantics/.build/defn/llvm/driver-kompiled`
spits out:

```
Line: 18529
/home/dwightguth/evm-semantics/./.build/defn/llvm/driver.k:493:10
```

This is the line of definition.kore that the axiom appears on as well as the
original location of the rule in the K semantics. You can use this information
to figure out which rules and functions are causing the most time and optimize
them to be more efficient.

